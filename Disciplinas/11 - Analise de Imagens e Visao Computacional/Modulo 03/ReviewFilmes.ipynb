{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd02c1855c28b8b597f057332dae11de67c6b939458ee52262e1ccdf6e51de96f0d",
   "display_name": "Python 3.8.8 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Neste exemplo você ira construir um classificador binário para o dataset IMDB (NLP Problem)\n",
    "\n",
    "O imdb dataset já vem no Keras e já é pre-processado. Os Reviews (sequencia de palavras) já estão organizados em sequências de inteiros, em que cada inteiro significa uma palavra específica do dicionário."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "source": [
    "As 10000 palavras mais frequentes serão mantidas no dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 1s 0us/step\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "D:\\Users\\angel\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "D:\\Users\\angel\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "source": [
    "train_data e test_data são as listas de reviews, cada review é uma lista de indices de palavras (texto do review codificado no indice de palavra)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "source": [
    "train_labels e test_labels são as listas de 0 e 1 (0=review negativo, 1=review positivo)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[0])"
   ]
  },
  {
   "source": [
    "Veja que nenhum indice da palavra ira exceder 10000"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "max([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "source": [
    "Não podemos inserir uma lista de numeros inteiros na rede. Temos que converter a lista para um tensor no formato (samples, word_indices).\n",
    "\n",
    "Por exemplo, transformar a sequencia \\[3,5\\] em um vetor de 10000 dimensões que seria todos seriam 0s, exceto os indices 3 e 5 seriam 1s."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequence(sequence, dimension=10000):\n",
    "    \n",
    "    # create an all-zeros matrix of shape (len(sequence), dimension)\n",
    "    results = np.zeros((len(sequence), dimension))\n",
    "\n",
    "    for i, sequence in enumerate(sequence):\n",
    "        results[i, sequence] = 1.\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorize_sequence(train_data)\n",
    "x_test = vectorize_sequence(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(train_labels).astype('float')\n",
    "y_test =  np.asarray(test_labels).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "source": [
    "**Construindo a rede**\n",
    "\n",
    "O input é um vetor e os labels são escalares (1s e 0s). Vamos usar um fully connected (Dense) com relu como activation function. Dense(16, activation='relu')\n",
    "\n",
    "16: é o argumento para cada dense layer. Ou seja, é o número de hidden units da camada (é a dimensão da representação da camada).\n",
    "\n",
    "**Configuração da rede**\n",
    "\n",
    "Iremos configurar a rede com duas camadas intermediarias com 16 unidades ocultas cada, que reproduzir-a a previsão escalar em relação ao sentimento do review em questão."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "source": [
    "Escolhendo a loss function e o optimizer (como strings por já fazerem parte do keras)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import losses, metrics\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "model.compile(optimizer=RMSprop(learning_rate=0.001), loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy])"
   ]
  },
  {
   "source": [
    "No entanto, vode pode deixar tudo como padrão"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])"
   ]
  },
  {
   "source": [
    "Para monitorar o treinamento, criamos o conjunto de validação separando 10000 amostras do conjunto original"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[10000:]\n",
    "partial_x_train = x_train[10000:]\n",
    "y_val = y_train[10000:]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "source": [
    "Iremos treinar o modelo por 20 epochs (20 interações sobre todas as amostras do treinamento) em um batch de 512 amostras"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.5412 - binary_accuracy: 0.7605 - val_loss: 0.3778 - val_binary_accuracy: 0.8873\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.3292 - binary_accuracy: 0.8993 - val_loss: 0.2733 - val_binary_accuracy: 0.9126\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2385 - binary_accuracy: 0.9253 - val_loss: 0.2222 - val_binary_accuracy: 0.9205\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.1862 - binary_accuracy: 0.9420 - val_loss: 0.1506 - val_binary_accuracy: 0.9581\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1527 - binary_accuracy: 0.9512 - val_loss: 0.1167 - val_binary_accuracy: 0.9703\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1249 - binary_accuracy: 0.9609 - val_loss: 0.1293 - val_binary_accuracy: 0.9554\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1047 - binary_accuracy: 0.9695 - val_loss: 0.0778 - val_binary_accuracy: 0.9828\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0879 - binary_accuracy: 0.9763 - val_loss: 0.0685 - val_binary_accuracy: 0.9853\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0698 - binary_accuracy: 0.9833 - val_loss: 0.0528 - val_binary_accuracy: 0.9889\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0604 - binary_accuracy: 0.9844 - val_loss: 0.0398 - val_binary_accuracy: 0.9928\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0500 - binary_accuracy: 0.9867 - val_loss: 0.0319 - val_binary_accuracy: 0.9948\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0333 - binary_accuracy: 0.9941 - val_loss: 0.0350 - val_binary_accuracy: 0.9940\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.0320 - binary_accuracy: 0.9936 - val_loss: 0.0189 - val_binary_accuracy: 0.9980\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0268 - binary_accuracy: 0.9943 - val_loss: 0.0143 - val_binary_accuracy: 0.9989\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0149 - binary_accuracy: 0.9987 - val_loss: 0.0207 - val_binary_accuracy: 0.9973\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.0154 - binary_accuracy: 0.9982 - val_loss: 0.0198 - val_binary_accuracy: 0.9972\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.0102 - binary_accuracy: 0.9993 - val_loss: 0.0104 - val_binary_accuracy: 0.9995\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0076 - binary_accuracy: 0.9995 - val_loss: 0.0215 - val_binary_accuracy: 0.9960\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.0057 - binary_accuracy: 0.9995 - val_loss: 0.0158 - val_binary_accuracy: 0.9981\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0045 - binary_accuracy: 0.9996 - val_loss: 0.0028 - val_binary_accuracy: 0.9998\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e618436130>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))"
   ]
  },
  {
   "source": [
    "**Usando uma rede treinada para gerar previsões sobre novos dados**\n",
    "\n",
    "Gerando a probabilidade de cada análise ser positiva com o método predict"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[3.9613247e-03]\n [9.9999875e-01]\n [6.4455426e-01]\n ...\n [9.1353059e-04]\n [4.8096180e-03]\n [5.6830722e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(x_test))"
   ]
  },
  {
   "source": [
    "A rede esta confiante para algumas amostras (0.99 ou mais, ou 0.01 ou menos), mas menos confiante para outras (0.6, 0.4)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Treinando o modelo do zero**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/4\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.4573 - acc: 0.8190\n",
      "Epoch 2/4\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.2594 - acc: 0.9103\n",
      "Epoch 3/4\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.2005 - acc: 0.9285\n",
      "Epoch 4/4\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1680 - acc: 0.9410\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2935 - acc: 0.8846\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.2934575080871582, 0.8845599889755249]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "source": [
    "Referênce: François Chollet. Deep Learning with Python. November 2017"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}