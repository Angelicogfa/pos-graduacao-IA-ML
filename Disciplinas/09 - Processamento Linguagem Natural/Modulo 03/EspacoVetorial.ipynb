{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "0204325da5c4cfa76f1d74fd4e08d4b1d3f65652f0a93ad9c8a4a57490d8536c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Espaço vetorial\n",
    "\n",
    "* A detecção de palavras no contexto de NLP é util para tarefas como obter estatísticas sobre o uso de palavras ou fazer pesquisas de palvra-chave.\n",
    "\n",
    "* Isso fará que um detector de spam seja menos propenso a se enganar com uma única palavra.\n",
    "\n",
    "* Ou até mesmo pode avaliar o quão positivo um tweet é quando há uma ampla variedade de palavras com vários graus de pontuação de \"positividade\"\n",
    "\n",
    "* A frequência com que essas palavras aparecem em um documento em relação ao restante dos documentos pode ser usada para refinar ainda mais a \"positividade\" do documento.\n",
    "\n",
    "* A ideia aqui é estudar medidas mais diferenciadas e seu uso em um documento.\n",
    "\n",
    "* A abordagem que analisaremos aqui tem sido a base para a geração de recursos da linguagem natural para mecanismos de busca comercial e filtros de spam por décadas.\n",
    "\n",
    "* A técnica de tokenização transforma palavras em números inteiros por represetar a ocorrência de cada palavra em um documento (vetores binarios).\n",
    "\n",
    "* A ideia agora e representar as palavras em um espaço contínuo.\n",
    "\n",
    "* Iremos analisar três técnicas de representar palavras e sua importância em um documento.\n",
    "\n",
    "    * Bag of words: vetores de frequência de palavras.\n",
    "\n",
    "    * Bag of n-grams: Contagem de pares (n) de palavras.\n",
    "\n",
    "    * TF-IDF vectors: pontuação da palavra que melhor representa sua importância.\n",
    "    "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* Assumimos que, **quanto mais vezes uma palavra ocorre no documento, maior deve ser o significado da palavra para o documento**."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['the', 'faster', 'harry', 'got', 'to', 'the', 'store', ',', 'the', 'faster', 'harry', ',', 'the', 'faster', ',', 'would', 'get', 'home', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The faster Harry got to the store, the faster Harry, the faster, would get home.\"\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(sentence.lower())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Counter({'the': 4, 'faster': 3, ',': 3, 'harry': 2, 'got': 1, 'to': 1, 'store': 1, 'would': 1, 'get': 1, 'home': 1, '.': 1})\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = Counter(tokens)\n",
    "print(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('the', 4), ('faster', 3), (',', 3), ('harry', 2)]\n"
     ]
    }
   ],
   "source": [
    "bag_of_words_most_common = bag_of_words.most_common(4)\n",
    "print(bag_of_words_most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "times_harry_appers = bag_of_words['harry']\n",
    "print(times_harry_appers)"
   ]
  },
  {
   "source": [
    "* Para documentos pequenos como este, a lista não ordenada de palavras pode conter muitas informações sobre a inteção original da frase.\n",
    "\n",
    "* E estas informações são suficientes para, por exemplo, detectar spam, calcular sentimentos e até detectar sarcasmo.\n",
    "\n",
    "* Este número de vezes em que uma palavra aparece em um documento é chamado (em inglês) de **term frequency** (TF).\n",
    "\n",
    "* Se normalizado, ela sera dividido pelo número de termos no documento (que será no máximo 1, se todas as palavras do documento forem iguais).\n",
    "\n",
    "* A normalização é importante porque, dependendo do tamanho do documento, uma palavra aparecr 100 vezes pode indicar muita relevância para um documento de 1000 (0.1) palavras ou indicar baixa relevância para um documento de 1000000 (0.001).\n",
    "\n",
    "* Assim, para cada palavra podemos calcular a importância relativa do documento neste termo.\n",
    "\n",
    "* Assim, obtemos os 4 principais termos daquele documento.\n",
    "\n",
    "* Sendo que termos como \"the\" e pontuações não são muito úteis para o documento e serão descartados (stop words).\n",
    "\n",
    "* Agora vamos calcular o TF da palavra \"harry\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.1818\n"
     ]
    }
   ],
   "source": [
    "unique_words = len(bag_of_words)\n",
    "tf = times_harry_appers / unique_words\n",
    "print(round(tf ,4))"
   ]
  },
  {
   "source": [
    "* Vamos utilizar um texto maior da wikipédia sobre pipas (kite_text da biblioteca \"nlpia\")."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# http://go.microsoft.com/fwlink/?LinkId=691126&fixForIE=.exe\n",
    "# !pip install nlpia --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:nlpia.constants:Starting logger in nlpia.constants...\n",
      "INFO:nlpia.loaders:No BIGDATA index found in D:\\angel\\anaconda3\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv so copy D:\\angel\\anaconda3\\lib\\site-packages\\nlpia\\data\\bigdata_info.latest.csv to D:\\angel\\anaconda3\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv if you want to \"freeze\" it.\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('D:\\\\angel\\\\anaconda3\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\mavis-batey-greetings.csv',), **{'low_memory': False})`...\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('D:\\\\angel\\\\anaconda3\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\sms-spam.csv',), **{'low_memory': False})`...\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nlpia.data.loaders import kite_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Counter({'the': 26, 'a': 20, 'kite': 16, ',': 15, 'and': 10, 'of': 10, 'kites': 8, 'is': 7, 'in': 7, 'or': 6, 'wing': 5, 'to': 5, 'be': 5, 'as': 5, 'lift': 4, 'have': 4, 'may': 4, 'at': 3, 'so': 3, 'can': 3, 'also': 3, 'kiting': 3, 'are': 3, 'flown': 3, 'tethered': 2, 'craft': 2, 'with': 2, 'that': 2, 'air': 2, 'consists': 2, 'tethers': 2, 'anchors.': 2, 'often': 2, 'bridle': 2, 'wind': 2, \"'s\": 2, 'designed': 2, ';': 2, 'when': 2, 'for': 2, 'moving': 2, 'technical': 2, 'even': 2, 'called': 2, 'surface': 2, 'pressure': 2, 'drag': 2, 'force': 2, 'by': 2, 'which': 2, '.': 2, 'used': 2, 'power': 2, 'traditionally': 1, 'heavier-than-air': 1, 'surfaces': 1, 'react': 1, 'against': 1, 'create': 1, 'drag.': 1, 'wings': 1, 'guide': 1, 'face': 1, 'correct': 1, 'angle': 1, 'it.': 1, 'not': 1, 'needed': 1, 'sailplane': 1, 'launch': 1, 'tether': 1, 'meets': 1, 'single': 1, 'point.': 1, 'fixed': 1, 'untraditionally': 1, 'tether-set-coupled': 1, 'sets': 1, 'though': 1, 'system': 1, 'still': 1, 'kite.': 1, 'sustains': 1, 'flight': 1, 'generated': 1, 'flows': 1, 'around': 1, 'producing': 1, 'low': 1, 'above': 1, 'high': 1, 'below': 1, 'wings.': 1, 'interaction': 1, 'generates': 1, 'horizontal': 1, 'along': 1, 'direction': 1, 'wind.': 1, 'resultant': 1, 'vector': 1, 'from': 1, 'components': 1, 'opposed': 1, 'tension': 1, 'one': 1, 'more': 1, 'lines': 1, 'attached.': 1, 'anchor': 1, 'point': 1, 'line': 1, 'static': 1, '(': 1, 'e.g.': 1, 'towing': 1, 'running': 1, 'person': 1, 'boat': 1, 'free-falling': 1, 'anchors': 1, 'paragliders': 1, 'fugitive': 1, 'parakites': 1, 'vehicle': 1, ')': 1, 'same': 1, 'principles': 1, 'fluid': 1, 'flow': 1, 'apply': 1, 'liquids': 1, 'under': 1, 'water.': 1, 'hybrid': 1, 'comprising': 1, 'both': 1, 'lighter-than-air': 1, 'balloon': 1, 'well': 1, 'lifting': 1, 'kytoon.': 1, 'long': 1, 'varied': 1, 'history': 1, 'many': 1, 'different': 1, 'types': 1, 'individually': 1, 'festivals': 1, 'worldwide.': 1, 'recreation': 1, 'art': 1, 'other': 1, 'practical': 1, 'uses.': 1, 'sport': 1, 'aerial': 1, 'ballet': 1, 'sometimes': 1, 'part': 1, 'competition.': 1, 'multi-line': 1, 'steerable': 1, 'generate': 1, 'large': 1, 'forces': 1, 'activities': 1, 'such': 1, 'surfing': 1, 'landboarding': 1, 'fishing': 1, 'buggying': 1, 'new': 1, 'trend': 1, 'snow': 1, 'kiting.': 1, 'man-lifting': 1, 'been': 1, 'made': 1})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(kite_text.lower())\n",
    "tokens_counts = Counter(tokens)\n",
    "print(tokens_counts)"
   ]
  },
  {
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nlpia.data.loaders import kite_text\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "tokens = tokenizer.tokenize(kite_text.lower())\n",
    "tokens_counts = Counter(tokens)\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "tokens = [x for x in tokens if x not in stopwords]\n",
    "kite_couts = Counter(tokens)\n",
    "print(kite_couts)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Counter({'kite': 16, ',': 15, 'kites': 8, 'wing': 5, 'lift': 4, 'may': 4, 'also': 3, 'kiting': 3, 'flown': 3, 'tethered': 2, 'craft': 2, 'air': 2, 'consists': 2, 'tethers': 2, 'anchors.': 2, 'often': 2, 'bridle': 2, 'wind': 2, \"'s\": 2, 'designed': 2, ';': 2, 'moving': 2, 'technical': 2, 'even': 2, 'called': 2, 'surface': 2, 'pressure': 2, 'drag': 2, 'force': 2, '.': 2, 'used': 2, 'power': 2, 'traditionally': 1, 'heavier-than-air': 1, 'surfaces': 1, 'react': 1, 'create': 1, 'drag.': 1, 'wings': 1, 'guide': 1, 'face': 1, 'correct': 1, 'angle': 1, 'it.': 1, 'needed': 1, 'sailplane': 1, 'launch': 1, 'tether': 1, 'meets': 1, 'single': 1, 'point.': 1, 'fixed': 1, 'untraditionally': 1, 'tether-set-coupled': 1, 'sets': 1, 'though': 1, 'system': 1, 'still': 1, 'kite.': 1, 'sustains': 1, 'flight': 1, 'generated': 1, 'flows': 1, 'around': 1, 'producing': 1, 'low': 1, 'high': 1, 'wings.': 1, 'interaction': 1, 'generates': 1, 'horizontal': 1, 'along': 1, 'direction': 1, 'wind.': 1, 'resultant': 1, 'vector': 1, 'components': 1, 'opposed': 1, 'tension': 1, 'one': 1, 'lines': 1, 'attached.': 1, 'anchor': 1, 'point': 1, 'line': 1, 'static': 1, '(': 1, 'e.g.': 1, 'towing': 1, 'running': 1, 'person': 1, 'boat': 1, 'free-falling': 1, 'anchors': 1, 'paragliders': 1, 'fugitive': 1, 'parakites': 1, 'vehicle': 1, ')': 1, 'principles': 1, 'fluid': 1, 'flow': 1, 'apply': 1, 'liquids': 1, 'water.': 1, 'hybrid': 1, 'comprising': 1, 'lighter-than-air': 1, 'balloon': 1, 'well': 1, 'lifting': 1, 'kytoon.': 1, 'long': 1, 'varied': 1, 'history': 1, 'many': 1, 'different': 1, 'types': 1, 'individually': 1, 'festivals': 1, 'worldwide.': 1, 'recreation': 1, 'art': 1, 'practical': 1, 'uses.': 1, 'sport': 1, 'aerial': 1, 'ballet': 1, 'sometimes': 1, 'part': 1, 'competition.': 1, 'multi-line': 1, 'steerable': 1, 'generate': 1, 'large': 1, 'forces': 1, 'activities': 1, 'surfing': 1, 'landboarding': 1, 'fishing': 1, 'buggying': 1, 'new': 1, 'trend': 1, 'snow': 1, 'kiting.': 1, 'man-lifting': 1, 'made': 1})\n"
     ]
    }
   ]
  },
  {
   "source": [
    "## Vetorização\n",
    "\n",
    "* Agora, ao invés de manter a descrição do documento com um dicionário de frequência, vamos produzir um vetor dessa contagem de palavras."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.07207207207207207, 0.06756756756756757, 0.036036036036036036, 0.02252252252252252, 0.018018018018018018, 0.018018018018018018, 0.013513513513513514, 0.013513513513513514, 0.013513513513513514, 0.009009009009009009, 0.009009009009009009, 0.009009009009009009, 0.009009009009009009, 0.009009009009009009, 0.009009009009009009, 0.009009009009009009, 0.009009009009009009, 0.009009009009009009, 0.009009009009009009, 0.009009009009009009, 0.009009009009009009, 0.009009009009009009, 0.009009009009009009, 0.009009009009009009, 0.009009009009009009, 0.009009009009009009, 0.009009009009009009, 0.009009009009009009, 0.009009009009009009, 0.009009009009009009, 0.009009009009009009, 0.009009009009009009, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045, 0.0045045045045045045]\n\n0.9999999999999953\n"
     ]
    }
   ],
   "source": [
    "document_vector = []\n",
    "doc_len = len(tokens)\n",
    "\n",
    "for key, value in kite_couts.most_common():\n",
    "    document_vector.append(value / doc_len)\n",
    "\n",
    "print(document_vector)\n",
    "print()\n",
    "print(sum(document_vector))"
   ]
  },
  {
   "source": [
    "* Cada dimensão deste vetor é o TF (term frequency) normalizado de cada palavra neste (específico) documento.\n",
    "\n",
    "* Mas observe que, como estamos tratando de apenas um único documento, todas as posições correspondem às palavras que aparecem no documento.\n",
    "\n",
    "* Normalmente, com vários documentos, como que noso vetor se comporta ?\n",
    "\n",
    "* Para comparar vários documento, precisamos que todos tenham vetores do mesmo tamanho (dimensão), sendo que o valor desta dimentsão é a quantidade total de palavras do vocabulário."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "docs = ['The faster Harry go to the store, the faster and faster Harry would get home.']\n",
    "docs.append('Harry is hairy and faster than Jill.')\n",
    "docs.append('Jill is not as hairy as Harry.')\n",
    "\n",
    "doc_tokes = []\n",
    "\n",
    "for doc in docs:\n",
    "    doc_tokes += [sorted(tokenizer.tokenize(doc.lower()))]\n",
    "\n",
    "print(len(doc_tokes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[',',\n",
       "  '.',\n",
       "  'and',\n",
       "  'faster',\n",
       "  'faster',\n",
       "  'faster',\n",
       "  'get',\n",
       "  'go',\n",
       "  'harry',\n",
       "  'harry',\n",
       "  'home',\n",
       "  'store',\n",
       "  'the',\n",
       "  'the',\n",
       "  'the',\n",
       "  'to',\n",
       "  'would'],\n",
       " ['.', 'and', 'faster', 'hairy', 'harry', 'is', 'jill', 'than'],\n",
       " ['.', 'as', 'as', 'hairy', 'harry', 'is', 'jill', 'not']]"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "doc_tokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doc_tokens = sum(doc_tokes, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "print(len(all_doc_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "lexicon = sorted(set(all_doc_tokens))\n",
    "print(len(lexicon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "OrderedDict([(',', 0),\n",
       "             ('.', 0),\n",
       "             ('and', 0),\n",
       "             ('as', 0),\n",
       "             ('faster', 0),\n",
       "             ('get', 0),\n",
       "             ('go', 0),\n",
       "             ('hairy', 0),\n",
       "             ('harry', 0),\n",
       "             ('home', 0),\n",
       "             ('is', 0),\n",
       "             ('jill', 0),\n",
       "             ('not', 0),\n",
       "             ('store', 0),\n",
       "             ('than', 0),\n",
       "             ('the', 0),\n",
       "             ('to', 0),\n",
       "             ('would', 0)])"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "zero_vector = OrderedDict((token, 0) for token in lexicon)\n",
    "zero_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "doc_vector = []\n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    tokens_count = Counter(tokens)\n",
    "    for key, valu in tokens_count.items():\n",
    "        vec[key] = valu / len(lexicon)\n",
    "    doc_vector.append(dict(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3\n{',': 0.05555555555555555, '.': 0.05555555555555555, 'and': 0.05555555555555555, 'as': 0, 'faster': 0.16666666666666666, 'get': 0.05555555555555555, 'go': 0.05555555555555555, 'hairy': 0, 'harry': 0.1111111111111111, 'home': 0.05555555555555555, 'is': 0, 'jill': 0, 'not': 0, 'store': 0.05555555555555555, 'than': 0, 'the': 0.16666666666666666, 'to': 0.05555555555555555, 'would': 0.05555555555555555}\n"
     ]
    }
   ],
   "source": [
    "print(len(doc_vector))\n",
    "print(doc_vector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}