{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "0204325da5c4cfa76f1d74fd4e08d4b1d3f65652f0a93ad9c8a4a57490d8536c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Espaço vetorial\n",
    "\n",
    "* A detecção de palavras no contexto de NLP é util para tarefas como obter estatísticas sobre o uso de palavras ou fazer pesquisas de palvra-chave.\n",
    "\n",
    "* Isso fará que um detector de spam seja menos propenso a se enganar com uma única palavra.\n",
    "\n",
    "* Ou até mesmo pode avaliar o quão positivo um tweet é quando há uma ampla variedade de palavras com vários graus de pontuação de \"positividade\"\n",
    "\n",
    "* A frequência com que essas palavras aparecem em um documento em relação ao restante dos documentos pode ser usada para refinar ainda mais a \"positividade\" do documento.\n",
    "\n",
    "* A ideia aqui é estudar medidas mais diferenciadas e seu uso em um documento.\n",
    "\n",
    "* A abordagem que analisaremos aqui tem sido a base para a geração de recursos da linguagem natural para mecanismos de busca comercial e filtros de spam por décadas.\n",
    "\n",
    "* A técnica de tokenização transforma palavras em números inteiros por represetar a ocorrência de cada palavra em um documento (vetores binarios).\n",
    "\n",
    "* A ideia agora e representar as palavras em um espaço contínuo.\n",
    "\n",
    "* Iremos analisar três técnicas de representar palavras e sua importância em um documento.\n",
    "\n",
    "    * Bag of words: vetores de frequência de palavras.\n",
    "\n",
    "    * Bag of n-grams: Contagem de pares (n) de palavras.\n",
    "\n",
    "    * TF-IDF vectors: pontuação da palavra que melhor representa sua importância.\n",
    "    "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* Assumimos que, **quanto mais vezes uma palavra ocorre no documento, maior deve ser o significado da palavra para o documento**."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['the', 'faster', 'harry', 'got', 'to', 'the', 'store', ',', 'the', 'faster', 'harry', ',', 'the', 'faster', ',', 'would', 'get', 'home', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The faster Harry got to the store, the faster Harry, the faster, would get home.\"\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(sentence.lower())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Counter({'the': 4, 'faster': 3, ',': 3, 'harry': 2, 'got': 1, 'to': 1, 'store': 1, 'would': 1, 'get': 1, 'home': 1, '.': 1})\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = Counter(tokens)\n",
    "print(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('the', 4), ('faster', 3), (',', 3), ('harry', 2)]\n"
     ]
    }
   ],
   "source": [
    "bag_of_words_most_common = bag_of_words.most_common(4)\n",
    "print(bag_of_words_most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "times_harry_appers = bag_of_words['harry']\n",
    "print(times_harry_appers)"
   ]
  },
  {
   "source": [
    "* Para documentos pequenos como este, a lista não ordenada de palavras pode conter muitas informações sobre a inteção original da frase.\n",
    "\n",
    "* E estas informações são suficientes para, por exemplo, detectar spam, calcular sentimentos e até detectar sarcasmo.\n",
    "\n",
    "* Este número de vezes em que uma palavra aparece em um documento é chamado (em inglês) de **term frequency** (TF).\n",
    "\n",
    "* Se normalizado, ela sera dividido pelo número de termos no documento (que será no máximo 1, se todas as palavras do documento forem iguais).\n",
    "\n",
    "* A normalização é importante porque, dependendo do tamanho do documento, uma palavra aparecr 100 vezes pode indicar muita relevância para um documento de 1000 (0.1) palavras ou indicar baixa relevância para um documento de 1000000 (0.001).\n",
    "\n",
    "* Assim, para cada palavra podemos calcular a importância relativa do documento neste termo.\n",
    "\n",
    "* Assim, obtemos os 4 principais termos daquele documento.\n",
    "\n",
    "* Sendo que termos como \"the\" e pontuações não são muito úteis para o documento e serão descartados (stop words).\n",
    "\n",
    "* Agora vamos calcular o TF da palavra \"harry\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.1818\n"
     ]
    }
   ],
   "source": [
    "unique_words = len(bag_of_words)\n",
    "tf = times_harry_appers / unique_words\n",
    "print(round(tf ,4))"
   ]
  },
  {
   "source": [
    "* Vamos utilizar um texto maior da wikipédia sobre pipas (kite_text da biblioteca \"nlpia\")."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nlpia'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-c0f849ee672a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTreebankWordTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnlpia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloaders\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkite_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nlpia'"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nlpia.data.loaders import kite_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'kite_text' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-0d8b4b5fa8c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTreebankWordTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkite_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtokens_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCouter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'kite_text' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(kite_text.lower())\n",
    "tokens_counts = Couter(tokens)\n",
    "print(tokens_counts)"
   ]
  },
  {
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nlpia.data.loaders import kite_text\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "tokens = tokenizer.tokenize(kite_text.lower())\n",
    "tokens_counts = Counter(tokens)\n",
    "\n",
    "nltk.download('stopword', quiet=True)\n",
    "stopwords = nltk.corpus.stopwords.words('enghish')\n",
    "\n",
    "tokens = [x for x in tokens if x not in stopwords]\n",
    "kite_couts = Counter(tokens)\n",
    "print(kite_couts)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nlpia'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-cc16e64a82cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTreebankWordTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnlpia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloaders\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkite_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nlpia'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}