{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Atividade-Pratica-2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"cells":[{"cell_type":"markdown","metadata":{"id":"QfNqR7LMKnF1"},"source":["# Atividade 02: Implementando uma Rede Neural\n","\n","Nesta atividade, você irá treinar uma rede neural com camadas completamente conectadas para realizar classificação de imagens, e irá testá-la utilizando o dataset CIFAR-10.\n","\n","Nesta atividade, você irá:\n","\n","- testar uma **função de perda** (**loss function**) para uma rede neural de duas camadas\n","- testar a avaliação de um **gradiente analítico**\n","- **verificar a implementação** utilizando gradiente numérico\n","- **treinar** uma rede em um pequeno problema por meio de **SGD**\n","- **treinar e depurar** uma rede em um conjunto de **dados reais**\n","- usar um conjunto de validação para **ajustar hiperparâmetros**\n","- **visualizar** os pesos finais que foram obtidos\n"]},{"cell_type":"markdown","metadata":{"id":"ef9_YhzcLHTY"},"source":["## Preparação para usar o *Colaboratory*\n","\n","Caso você deseje usar o *Colaborabory*, execute os comandos da célula a seguir para conseguir acessar os arquivos em uma conta do **Google Drive**.\n"]},{"cell_type":"code","metadata":{"id":"kzynBb0jLHxc"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6fOr9nVLMWeM"},"source":["Além disso, você talvez deseje trabalhar um subdiretório (ou pasta) específico, por exemplo `Pratica2`.  Esse diretório (ou pasta) já deve ter sido criado e conter o ***notebook*** dessa prática.\n","\n","Sendo assim, utilize as instruções na próxima célula para alterar o diretório corrente para esse subdiretório (alterando o nome do subdiretório, se for o caso)."]},{"cell_type":"code","metadata":{"id":"0jVoCqmCNf-3"},"source":["import os\n","os.chdir(\"drive/My Drive/Pratica2\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PC9_lPEaOJO5"},"source":["## Obtendo o conjunto de dados CIFAR-10\n","\n","Nesta atividade, você irá desenvolver uma rede neural para realizar classificação de imagens e irá testá-la utilizando o dataset CIFAR-10. \n","\n","Para tando, você deve obter as imagens dessa base de dados executando a célula a seguir.\n","\n","**OBS: Você só precisar realizar esta etapa uma vez!** Caso você já a tenha feito anteriormente e está retornando para continuar a execução dessa prática, não será necessário que se faça a recuperação das imagens novamente. Para verificar se os dados da base estão disponíveis, basta checar a existência do subdiretório `cifar-10-batches-py` no diretório corrente."]},{"cell_type":"code","metadata":{"id":"K9t2Rkk-KWcu"},"source":["!wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","!tar -xzvf cifar-10-python.tar.gz\n","!rm cifar-10-python.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fh1-o9BsV9Gs"},"source":["## Código de inicialização e configuração básica\n","\n","Execute a célula a seguir para garantir a importação de alguns recursos básicos, bem como a inicialização/configuração para exibição correta de gráficos. "]},{"cell_type":"code","metadata":{"id":"vka0RPn4QI_u"},"source":["# Algum código de inicialização\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from past.builtins import xrange\n","\n","from __future__ import print_function\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0)\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","def rel_error(x, y):\n","    \"\"\" returna erro relativo \"\"\"\n","    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0bkUO7brS4C3"},"source":["## Código para Classe implementando Rede Neural com 02 Camadas\n","\n","Nesta atividade será utilizada a classe `TwoLayerNet`  para representar instâncias de uma rede neural. Esta classe se encontra definida dentro da célula abaixo .\n","\n","Os parâmetros da rede serão armazenados na variável de instância `self.params` que é um dicionário em que as chaves são os nomes (*strings*) de cada parâmetro e os valores são **arrays numpy**.\n","\n"]},{"cell_type":"code","metadata":{"id":"jc0FwXQQSIdd"},"source":["class TwoLayerNet(object):\n","  \"\"\"\n","  A two-layer fully-connected neural network. The net has an input dimension of\n","  N, a hidden layer dimension of H, and performs classification over C classes.\n","  We train the network with a softmax loss function and L2 regularization on the\n","  weight matrices. The network uses a ReLU nonlinearity after the first fully\n","  connected layer.\n","\n","  In other words, the network has the following architecture:\n","\n","  input - fully connected layer - ReLU - fully connected layer - softmax\n","\n","  The outputs of the second fully-connected layer are the scores for each class.\n","  \"\"\"\n","\n","  def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n","    \"\"\"\n","    Initialize the model. Weights are initialized to small random values and\n","    biases are initialized to zero. Weights and biases are stored in the\n","    variable self.params, which is a dictionary with the following keys:\n","\n","    W1: First layer weights; has shape (D, H)\n","    b1: First layer biases; has shape (H,)\n","    W2: Second layer weights; has shape (H, C)\n","    b2: Second layer biases; has shape (C,)\n","\n","    Inputs:\n","    - input_size: The dimension D of the input data.\n","    - hidden_size: The number of neurons H in the hidden layer.\n","    - output_size: The number of classes C.\n","    \"\"\"\n","    self.params = {}\n","    self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n","    self.params['b1'] = np.zeros(hidden_size)\n","    self.params['W2'] = std * np.random.randn(hidden_size, output_size)\n","    self.params['b2'] = np.zeros(output_size)\n","\n","  def loss(self, X, y=None, reg=0.0):\n","    \"\"\"\n","    Compute the loss and gradients for a two layer fully connected neural\n","    network.\n","\n","    Inputs:\n","    - X: Input data of shape (N, D). Each X[i] is a training sample.\n","    - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is\n","      an integer in the range 0 <= y[i] < C. This parameter is optional; if it\n","      is not passed then we only return scores, and if it is passed then we\n","      instead return the loss and gradients.\n","    - reg: Regularization strength.\n","\n","    Returns:\n","    If y is None, return a matrix scores of shape (N, C) where scores[i, c] is\n","    the score for class c on input X[i].\n","\n","    If y is not None, instead return a tuple of:\n","    - loss: Loss (data loss and regularization loss) for this batch of training\n","      samples.\n","    - grads: Dictionary mapping parameter names to gradients of those parameters\n","      with respect to the loss function; has the same keys as self.params.\n","    \"\"\"\n","    # Unpack variables from the params dictionary\n","    W1, b1 = self.params['W1'], self.params['b1']\n","    W2, b2 = self.params['W2'], self.params['b2']\n","    N, D = X.shape\n","\n","    # Compute the forward pass\n","\n","    layer1 = X.dot(W1) + b1      # Forward 1st layer\n","    layer1[layer1<0] = 0         # ReLU\n","    layer2 = layer1.dot(W2) + b2 # Forward 2nd layer\n","    scores = layer2 # This seems correct, but maybe check why ReLU only on first layer\n","    \n","    # If the targets are not given then jump out, we're done\n","    if y is None:\n","      return scores\n","\n","    # Compute the loss\n","    loss = None\n","\n","    # N is the Number of samples in training batch\n","    maxScores = scores.max()\n","    fj = (scores - maxScores).T      # normalize Scores\n","    fyi = fj[y,range(N)]             # correct scores\n","\n","    # Li = -np.log( np.exp(fyi)/np.sum(np.exp(fj), axis=0))\n","    # or alternatively -- equivalent expressions\n","    Li = -fyi + np.log(np.sum(np.exp(fj), axis=0))\n","\n","    mu_Li = np.mean( Li )                               # dataLoss\n","    RW = 0.5 * reg * (np.sum(W1*W1) + np.sum(W2*W2))    # regLoss\n","\n","    loss = mu_Li + RW\n","\n","    # Backward pass: compute gradients\n","    grads = {}\n","\n","    dfj = (np.exp(fj)/np.sum(np.exp(fj), axis = 0)).T\n","    dfyi = np.zeros(fj.shape)\n","    dfyi[y,range(N)] = 1\n","\n","    dfj = (np.exp(fj)/np.sum(np.exp(fj), axis = 0))\n","    dLi = (dfj - dfyi)/N\n","\n","    db2 = dLi.sum(axis=1)\n","    dW2 = dLi.dot(layer1).T + reg*W2\n","\n","    dLayer1 = dLi.T.dot(W2.T)\n","    dLayer1[layer1<=0] = 0\n","\n","    db1 = dLayer1.sum(axis=0)\n","    dW1 = dLayer1.T.dot(X).T + reg*W1\n","\n","    assert db2.shape==b2.shape\n","    assert dW2.shape==W2.shape\n","    assert db1.shape==b1.shape\n","    assert dW1.shape==W1.shape\n","\n","    grads = {\n","        'b2': db2,\n","        'W2': dW2,\n","        'b1': db1,\n","        'W1': dW1\n","    }\n","\n","    return loss, grads\n","\n","  def train(self, X, y, X_val, y_val,\n","            learning_rate=1e-3, learning_rate_decay=0.95,\n","            reg=5e-6, num_iters=100,\n","            batch_size=200, verbose=False):\n","    \"\"\"\n","    Train this neural network using stochastic gradient descent.\n","\n","    Inputs:\n","    - X: A numpy array of shape (N, D) giving training data.\n","    - y: A numpy array f shape (N,) giving training labels; y[i] = c means that\n","      X[i] has label c, where 0 <= c < C.\n","    - X_val: A numpy array of shape (N_val, D) giving validation data.\n","    - y_val: A numpy array of shape (N_val,) giving validation labels.\n","    - learning_rate: Scalar giving learning rate for optimization.\n","    - learning_rate_decay: Scalar giving factor used to decay the learning rate\n","      after each epoch.\n","    - reg: Scalar giving regularization strength.\n","    - num_iters: Number of steps to take when optimizing.\n","    - batch_size: Number of training examples to use per step.\n","    - verbose: boolean; if true print progress during optimization.\n","    \"\"\"\n","    num_train = X.shape[0]\n","    iterations_per_epoch = max(num_train / batch_size, 1)\n","\n","    # Use SGD to optimize the parameters in self.model\n","    loss_history = []\n","    train_acc_history = []\n","    val_acc_history = []\n","\n","    idxItems = range(num_train)\n","\n","    for it in xrange(num_iters):\n","      X_batch = None\n","      y_batch = None\n","\n","      selectIdx = np.random.choice(idxItems, size=batch_size, replace=True)\n","      X_batch = X[selectIdx,:]\n","      y_batch = y[selectIdx]\n","\n","      # Compute loss and gradients using the current minibatch\n","      loss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n","      loss_history.append(loss)\n","\n","      for paramName in self.params:\n","          self.params[paramName] -= learning_rate * grads[paramName]\n","\n","      if verbose and it % 100 == 0:\n","        print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n","\n","      # Every epoch, check train and val accuracy and decay learning rate.\n","      if it % iterations_per_epoch == 0:\n","        # Check accuracy\n","        train_acc = (self.predict(X_batch) == y_batch).mean()\n","        val_acc = (self.predict(X_val) == y_val).mean()\n","        train_acc_history.append(train_acc)\n","        val_acc_history.append(val_acc)\n","\n","        # Decay learning rate\n","        learning_rate *= learning_rate_decay\n","\n","    return {\n","      'loss_history': loss_history,\n","      'train_acc_history': train_acc_history,\n","      'val_acc_history': val_acc_history,\n","    }\n","\n","  def predict(self, X):\n","    \"\"\"\n","    Use the trained weights of this two-layer network to predict labels for\n","    data points. For each data point we predict scores for each of the C\n","    classes, and assign each data point to the class with the highest score.\n","\n","    Inputs:\n","    - X: A numpy array of shape (N, D) giving N D-dimensional data points to\n","      classify.\n","\n","    Returns:\n","    - y_pred: A numpy array of shape (N,) giving predicted labels for each of\n","      the elements of X. For all i, y_pred[i] = c means that X[i] is predicted\n","      to have class c, where 0 <= c < C.\n","    \"\"\"\n","    y_pred = self.loss(X, y=None, reg=0.0).argmax(axis=1)\n","\n","    return y_pred"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ryCLMZS8TQFb"},"source":["## Criação de Pequeno Exemplo (*Toy Model*) para Testes Preliminares\n","\n","A seguir, você irá inicializar um pequeno conjunto de dados e um modelo simples que será usado para iniciar os teste dessa implementação. "]},{"cell_type":"code","metadata":{"id":"kXIaZeZCTQ-u"},"source":["# Cria um pequeno conjunto de dados aleatórios e um modelo simples para verificar sua implementação.\n","# Veja que foi fixado a 'semente aleatória' para possibilitar a repetição de experimentos\n","\n","input_size = 4\n","hidden_size = 10\n","num_classes = 3\n","num_inputs = 5\n","\n","def init_toy_model():\n","    np.random.seed(0)\n","    return TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n","\n","def init_toy_data():\n","    np.random.seed(1)\n","    X = 10 * np.random.randn(num_inputs, input_size)\n","    y = np.array([0, 1, 2, 2, 1])\n","    return X, y\n","\n","net = init_toy_model()\n","X, y = init_toy_data()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yc_FJ3Ji-LyD"},"source":["# Passo de Propagação (*Forward pass*): cálculo de *scores*\n","Retorne ao código da classe `TwoLayerNet` e analise o método `TwoLayerNet.loss`. \n","\n","Esta função é muito similar as funções de perda que foram discutidas em sala de aula: ela utiliza os dados e os pesos (ou parâmetros) para calcular os *scores* de cada classe, o valor de perda/custo e os gradientes em relação aos parâmetros.\n","\n","Você deve examinar com cuidado a primeira parte do passo de propagação (*forward pass*) que utiliza os pesos e vieses (*biases*) para calcular os *scores* para todas as entradas."]},{"cell_type":"code","metadata":{"id":"PKMgFef5-LyE"},"source":["scores = net.loss(X)\n","print('Your scores:')\n","print(scores)\n","print()\n","print('correct scores:')\n","correct_scores = np.asarray([\n","  [-0.81233741, -1.27654624, -0.70335995],\n","  [-0.17129677, -1.18803311, -0.47310444],\n","  [-0.51590475, -1.01354314, -0.8504215 ],\n","  [-0.15419291, -0.48629638, -0.52901952],\n","  [-0.00618733, -0.12435261, -0.15226949]])\n","print(correct_scores)\n","print()\n","\n","# A diferença deve ser bem pequena, algo < 1e-7\n","print('Difference between your scores and correct scores:')\n","print(np.sum(np.abs(scores - correct_scores)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oiGM_h1X-LyH"},"source":["# Passo de Propagação (*Forward pass*): cálculo da perda/custo\n","Na mesma função, você deve examinar (o código e testar) a segunda parte do passo de propagação (*forward pass*) responsável pelo cálculo da perda envolvendo os dados e a regularização."]},{"cell_type":"code","metadata":{"id":"2Vzrf5mD-LyI"},"source":["loss, _ = net.loss(X, y, reg=0.1)\n","correct_loss = 1.30378789133\n","\n","# Novamente, a diferença deve ser pequena, algo < 1e-12\n","print('Difference between your loss and correct loss:')\n","print(np.sum(np.abs(loss - correct_loss)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2cKjxmrJ-LyM"},"source":["# Passo de Retropropagação (*Backward pass*)\n","Agora, você deve examinar o restante da função `TwoLayerNet.loss`, de modo que a compreender como a função calcula o gradiente da perda em relação aos parâmetros `W1`, `b1`, `W2` e `b2`.\n","\n","Agora, uma vez que você pode examinar em detalhes (e depurar) o passo de retropropagação: por meio do uso de estimativas numéricas do gradiente."]},{"cell_type":"code","metadata":{"id":"owNPbkn9-LyM"},"source":["from random import randrange\n","\n","def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n","  \"\"\" \n","  a naive implementation of numerical gradient of f at x \n","  - f should be a function that takes a single argument\n","  - x is the point (numpy array) to evaluate the gradient at\n","  \"\"\" \n","\n","  fx = f(x) # evaluate function value at original point\n","  grad = np.zeros_like(x)\n","  # iterate over all indexes in x\n","  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n","  while not it.finished:\n","\n","    # evaluate function at x+h\n","    ix = it.multi_index\n","    oldval = x[ix]\n","    x[ix] = oldval + h # increment by h\n","    fxph = f(x) # evalute f(x + h)\n","    x[ix] = oldval - h\n","    fxmh = f(x) # evaluate f(x - h)\n","    x[ix] = oldval # restore\n","\n","    # compute the partial derivative with centered formula\n","    grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n","    if verbose:\n","      print(ix, grad[ix])\n","    it.iternext() # step to next dimension\n","\n","  return grad\n","\n","# Usa verificação numérica do gradiente para checar sua implementação do passo de retropropagação.\n","# Se sua implementação estiver correta, a diferença entre a estimativa numérica do gradiente\n","# e o valor obtido analiticamente deve ser menor que 1e-8 para cada um dor parâmetros.\n","\n","loss, grads = net.loss(X, y, reg=0.05)\n","\n","# O erros deve ser menores que 1e-8\n","for param_name in grads:\n","    f = lambda W: net.loss(X, y, reg=0.05)[0]\n","    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n","    print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N6DDZtPk-LyQ"},"source":["# Treinamento de uma Rede Simples\n","Para treinar uma rede, você deve usar o método SGD (método de descida mais íngreme estocástico).\n","\n","Analise a função `TwoLayerNet.train` e examinando todas as partes que importantes para se implementar o procedimento de treinamento. \n","\n","Você também deve realizar uma análise da função `TwoLayerNet.predict` pois ela será necessária durante o treinamento, uma vez que periodicamente o método realiza predições para acompanhar a acurácia ao longo do processo.\n","\n","Uma vez que você tenha estudado tais métodos, execute o código abaixo para treinar um rede de duas camadas sobre o pequeno conjunto de dados aletários. Você deverá obter uma perda ao final do treinamento inferior a 0.2"]},{"cell_type":"code","metadata":{"id":"pWnZHvk9-LyQ"},"source":["net = init_toy_model()\n","stats = net.train(X, y, X, y,\n","            learning_rate=1e-1, reg=5e-6,\n","            num_iters=100, verbose=False)\n","\n","print('Final training loss: ', stats['loss_history'][-1])\n","\n","# Plota o histórico da função de perda\n","plt.plot(stats['loss_history'])\n","plt.xlabel('iteration')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B0fWuWHV-LyS"},"source":["# Carregamento de dados\n","Agora que você testou uma rede de duas camadas passando pela verificação de gradientes e seu funcionado sobre o pequeno conjunto de dados, é hora de carregar os dados do **CIFAR-10 dataset** de modo que você possa usá-los no treinamento de um classificador sobre dados reais."]},{"cell_type":"code","metadata":{"id":"iG-QjRaV-LyS"},"source":["from __future__ import print_function\n","\n","from six.moves import cPickle as pickle\n","import numpy as np\n","import os\n","import platform\n","\n","def load_pickle(f):\n","    version = platform.python_version_tuple()\n","    if version[0] == '2':\n","        return  pickle.load(f)\n","    elif version[0] == '3':\n","        return  pickle.load(f, encoding='latin1')\n","    raise ValueError(\"invalid python version: {}\".format(version))\n","\n","def load_CIFAR_batch(filename):\n","  \"\"\" load single batch of cifar \"\"\"\n","  with open(filename, 'rb') as f:\n","    datadict = load_pickle(f)\n","    X = datadict['data']\n","    Y = datadict['labels']\n","    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n","    Y = np.array(Y)\n","    return X, Y\n","\n","def load_CIFAR10(ROOT):\n","  \"\"\" load all of cifar \"\"\"\n","  xs = []\n","  ys = []\n","  for b in range(1,6):\n","    f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n","    X, Y = load_CIFAR_batch(f)\n","    xs.append(X)\n","    ys.append(Y)    \n","  Xtr = np.concatenate(xs)\n","  Ytr = np.concatenate(ys)\n","  del X, Y\n","  Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n","  return Xtr, Ytr, Xte, Yte\n","\n","def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n","    \"\"\"\n","    Carrega CIFAR-10 dataset a partit do disco e realiza preprocessamento para preparar\n","    os dados para a rede neural de duas camadas. Estes são os mesmos passos usados para o modelo\n","    SVM, porém condensado em uma única função.  \n","    \"\"\"\n","    # Carregga os dados CIFAR-10 brutos\n","    cifar10_dir = 'cifar-10-batches-py'\n","    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n","        \n","    # Subdivide os dados em conjuntos\n","    mask = list(range(num_training, num_training + num_validation))\n","    X_val = X_train[mask]\n","    y_val = y_train[mask]\n","    mask = list(range(num_training))\n","    X_train = X_train[mask]\n","    y_train = y_train[mask]\n","    mask = list(range(num_test))\n","    X_test = X_test[mask]\n","    y_test = y_test[mask]\n","\n","    return X_train, y_train, X_val, y_val, X_test, y_test\n","\n","\n","# Usar a função definida acmina para obter os dados.\n","X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n","print('Train data shape: ', X_train.shape)\n","print('Train labels shape: ', y_train.shape)\n","print('Validation data shape: ', X_val.shape)\n","print('Validation labels shape: ', y_val.shape)\n","print('Test data shape: ', X_test.shape)\n","print('Test labels shape: ', y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SuwsAE8Yjdpo"},"source":["## Visualizando amostras dos dados\n","Utilize a célula a seguir para visualizar algumas amostras das 10 classes de imagens da base `CIFAR-10`"]},{"cell_type":"code","metadata":{"id":"yyzh-dinbN4f"},"source":["# Visualizar alguns exemplos do dataset.\n","# São exibidos apenas 7 exemplos de imagens de treinamento de cada classe.\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","num_classes = len(classes)\n","samples_per_class = 7\n","for y, cls in enumerate(classes):\n","    idxs = np.flatnonzero(y_train == y)\n","    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n","    for i, idx in enumerate(idxs):\n","        plt_idx = i * num_classes + y + 1\n","        plt.subplot(samples_per_class, num_classes, plt_idx)\n","        plt.imshow(X_train[idx].astype('uint8'))\n","        plt.axis('off')\n","        if i == 0:\n","            plt.title(cls)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XFSUqX68jc4R"},"source":["## Normalização de dados\n","Conforme discutido em sala de aula é importante para facilitar o treinamento que seja feito algum tipo de tratamento dos dados. Neste caso, uma simples subtração de uma imagem média está sendo utilizada na célula a seguir, de modo a centralizar os dados em torno da origem do sistemas de coordenadas."]},{"cell_type":"code","metadata":{"id":"IjkA50hUcgB_"},"source":["# Normaliza os dados: subtrai a imagem média\n","mean_image = np.mean(X_train, axis=0)\n","X_train -= mean_image\n","X_val -= mean_image\n","X_test -= mean_image\n","\n","# Redimensiona as imagens de matrizes para vetores\n","X_train = X_train.reshape(49000, -1)\n","X_val = X_val.reshape(1000, -1)\n","X_test = X_test.reshape(1000, -1)\n","\n","print('Train data shape: ', X_train.shape)\n","print('Train labels shape: ', y_train.shape)\n","print('Validation data shape: ', X_val.shape)\n","print('Validation labels shape: ', y_val.shape)\n","print('Test data shape: ', X_test.shape)\n","print('Test labels shape: ', y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eGZWZXY3-LyU"},"source":["# Treinamento de uma Rede com Dados Reais\n","Para treinar sua rede, você deve usar **SGD**. Além disso, nesse processo a taxa de aprendizado será ajustada com um decaimento exponencial ao longo do processo de otimização, isto é, após cada época, a taxa de aprendizado é multiplicada pela taxa de decaimento (como esta última é menor que um, consequentemente a taxa de aprendizado é reduzida)."]},{"cell_type":"code","metadata":{"id":"uGPrOnfZ-LyW"},"source":["input_size = 32 * 32 * 3\n","hidden_size = 50\n","num_classes = 10\n","net = TwoLayerNet(input_size, hidden_size, num_classes)\n","\n","# Treinamento da rede\n","stats = net.train(X_train, y_train, X_val, y_val,\n","            num_iters=1000, batch_size=200,\n","            learning_rate=1e-4, learning_rate_decay=0.95,\n","            reg=0.25, verbose=True)\n","\n","# Predição sobre o conjunto de validação\n","val_acc = (net.predict(X_val) == y_val).mean()\n","print('Validation accuracy: ', val_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8ImHlrH2-LyX"},"source":["# Depuração do treinamento\n","Com os valores de parâmetros fornecidos acima, você deve ter obtido uma acurácia no conjunto de validação em torno de 0.29. O que não representa um resultado muito bom...\n","\n","Uma estratégia para melhorar o entendimento (fornecer *insigths*) sobre o que pode estar errado é traçar os gráficos de evolução da função de perda e das acurácias de treinamento e validação ao longo do processo de otimização."]},{"cell_type":"code","metadata":{"id":"g5ChCFM6-LyY"},"source":["# Plota a funcão de pedar e as acurácias de treinamento e validação\n","plt.subplot(2, 1, 1)\n","plt.plot(stats['loss_history'])\n","plt.title('Loss history')\n","plt.xlabel('Iteration')\n","plt.ylabel('Loss')\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(stats['train_acc_history'], label='train')\n","plt.plot(stats['val_acc_history'], label='val')\n","plt.title('Classification accuracy history')\n","plt.xlabel('Epoch')\n","plt.ylabel('Clasification accuracy')\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ExbsAAai-Lya"},"source":["Uma outra estratégia é construis uma visualização dos pesos que foram obtidos na primeira camada da rede. Por trás disto, está o fato de que na maioria das redes neurais treinadas sobre dados visuais, os pesos da primeira camada geralmente exibem algum tipo de estrutura visível."]},{"cell_type":"code","metadata":{"id":"6BoqDVHp-Lya"},"source":["from past.builtins import xrange\n","\n","from math import sqrt, ceil\n","import numpy as np\n","\n","def visualize_grid(Xs, ubound=255.0, padding=1):\n","  \"\"\"\n","  Reshape a 4D tensor of image data to a grid for easy visualization.\n","\n","  Inputs:\n","  - Xs: Data of shape (N, H, W, C)\n","  - ubound: Output grid will have values scaled to the range [0, ubound]\n","  - padding: The number of blank pixels between elements of the grid\n","  \"\"\"\n","  (N, H, W, C) = Xs.shape\n","  grid_size = int(ceil(sqrt(N)))\n","  grid_height = H * grid_size + padding * (grid_size - 1)\n","  grid_width = W * grid_size + padding * (grid_size - 1)\n","  grid = np.zeros((grid_height, grid_width, C))\n","  next_idx = 0\n","  y0, y1 = 0, H\n","  for y in xrange(grid_size):\n","    x0, x1 = 0, W\n","    for x in xrange(grid_size):\n","      if next_idx < N:\n","        img = Xs[next_idx]\n","        low, high = np.min(img), np.max(img)\n","        grid[y0:y1, x0:x1] = ubound * (img - low) / (high - low)\n","        # grid[y0:y1, x0:x1] = Xs[next_idx]\n","        next_idx += 1\n","      x0 += W + padding\n","      x1 += W + padding\n","    y0 += H + padding\n","    y1 += H + padding\n","  # grid_max = np.max(grid)\n","  # grid_min = np.min(grid)\n","  # grid = ubound * (grid - grid_min) / (grid_max - grid_min)\n","  return grid\n","\n","def vis_grid(Xs):\n","  \"\"\" visualize a grid of images \"\"\"\n","  (N, H, W, C) = Xs.shape\n","  A = int(ceil(sqrt(N)))\n","  G = np.ones((A*H+A, A*W+A, C), Xs.dtype)\n","  G *= np.min(Xs)\n","  n = 0\n","  for y in range(A):\n","    for x in range(A):\n","      if n < N:\n","        G[y*H+y:(y+1)*H+y, x*W+x:(x+1)*W+x, :] = Xs[n,:,:,:]\n","        n += 1\n","  # normalize to [0,1]\n","  maxg = G.max()\n","  ming = G.min()\n","  G = (G - ming)/(maxg-ming)\n","  return G\n","  \n","def vis_nn(rows):\n","  \"\"\" visualize array of arrays of images \"\"\"\n","  N = len(rows)\n","  D = len(rows[0])\n","  H,W,C = rows[0][0].shape\n","  Xs = rows[0][0]\n","  G = np.ones((N*H+N, D*W+D, C), Xs.dtype)\n","  for y in range(N):\n","    for x in range(D):\n","      G[y*H+y:(y+1)*H+y, x*W+x:(x+1)*W+x, :] = rows[y][x]\n","  # normalize to [0,1]\n","  maxg = G.max()\n","  ming = G.min()\n","  G = (G - ming)/(maxg-ming)\n","  return G\n","\n","# Visualiza os pesos da primeira camada da rede\n","\n","def show_net_weights(net):\n","    W1 = net.params['W1']\n","    W1 = W1.reshape(32, 32, 3, -1).transpose(3, 0, 1, 2)\n","    plt.imshow(visualize_grid(W1, padding=3).astype('uint8'))\n","    plt.gca().axis('off')\n","    plt.show()\n","\n","show_net_weights(net)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ykONhHSi-Lyc"},"source":["# Ajuste de hiperparâmetros\n","\n","**O que está errado?** Observando os gráficos e as visualizações acima, pode-se ver que a perda está reduzindo de forma *mais ou menos* linear, o que parece sugerir que a taxa de aprendizado pode estar muito baixa. Além disso, não há uma separação grande entre as acurácias de treinamento e validação, sugerindo que o modelo usado tem baixa capacidade e que talvez seu tamanho devesse ser aumentado. Por outro lado, com um modelo muito grande deve-se esperar observar mais *overfitting*, que se manifesta por meio de uma distância muita grande entre acurácias de treinamento e de validação.\n","\n","**Ajuste**. Realizar o ajuste de hiperparâmetros e desenvolver uma nocão intuitiva de como eles afetam o resultado final é uma parte importatnte do uso de Redes Neurais. Dessa forma, deseja-se que você realize várias práticas envolvendo o ajuste de hiperparâmetros. A seguir, você deve realizar experimentos com diferentes valores para os hiperparâmetros incluindo: tamanho da camada escondida, taxa de aprendizado, taxa de decaimento, número de épocas de treinamento e regularização. \n","\n","**Resultados aproximados**. Você deve tentar alcançar uma acurácia de classificação (taxa de acerto) maior que 48% no conjunto de validação. (OBS: minha solução obteve uma acurácia acima de 52% no conjunto de validação!)\n","\n","**Experimento**: O objetivo desse exercício é que você tente obter o melhor resultado sobre a base CIFAR-10, usando uma rede neural completamente conectada. Sinta-se livre para implementar quaisquer técnicas que desejar (p.ex., redução de dimensionalidade via PCA, *dropout*, ou outras estratégias ao otimizador, etc).\n","\n","**OBS: Lembre-se de deixar documentado tudo que foi feito! Caso necessário, acrescente mais células a sua vontade.**"]},{"cell_type":"code","metadata":{"id":"kBJjwPuJ-Lyd"},"source":["best_net = None # Armazenar o melhor modelo encontrado nesse variável pois será \n","                # usado no teste final (ver final desse notebook)\n","\n","#################################################################################\n","# TODO: Ajustar hiperparâmetros usando o conjunto de validação. O melhor modelo #\n","# obtido ao longo do treinamento deve-se armazenado em best_net.                #\n","#                                                                               #\n","# Para auxiliar a depurar sua rede, pode ser interessante se utilizar de        #\n","# visualizações similares as usadas acima. Essas visualizações irão apresentar  #\n","# diferenças qualitativas significativas especialmente para redes com ajustes   #\n","# ruins.                                                                        #\n","#                                                                               #\n","# O ajuste fino de hiperparâmetros feito manualmente pode ser divertido, mas    #\n","# provavelmente você deverá considerar a possibilidade de escrever código que   #\n","# percorra todas as combinações possíveis de hiperparâmetros de forma a tornar  #\n","# automático o processo de busca (similar ao que foi feito nas atividades       #\n","# anteriores).                                                                  #\n","#################################################################################\n","\n","pass\n","\n","#################################################################################\n","#                              FIM DE SEU CÓDIGO                               #\n","#################################################################################"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gtCbmnHzwo5c"},"source":["## Representando graficamente os resultados obtidos"]},{"cell_type":"code","metadata":{"id":"BQwHQUXZwfzY"},"source":["# Plota a funcão de pedar e as acurácias de treinamento e validação\n","plt.subplot(2, 1, 1)\n","plt.plot(stats['loss_history'])\n","plt.title('Loss history')\n","plt.xlabel('Iteration')\n","plt.ylabel('Loss')\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(stats['train_acc_history'], label='train')\n","plt.plot(stats['val_acc_history'], label='val')\n","plt.title('Classification accuracy history')\n","plt.xlabel('Epoch')\n","plt.ylabel('Clasification accuracy')\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6E2RRAZ4wr09"},"source":["## Visualizando pesos da primeira camada"]},{"cell_type":"code","metadata":{"id":"7WZEnzA0-Lyg"},"source":["# visualizar os pesos da primeira camada da melhor rede obtida\n","show_net_weights(best_net)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8qWROyoK-Lyj"},"source":["# Executar predições sobre o conjunto de teste\n","Quando você terminar com seus experimentos acima (**nunca antes!!!**), você deve avaliar sua rede final sobre o conjunto de teste e o resultado de acurácia (taxa de acerto) deve ser acima de 48%.\n","\n","**Você deve tentar encontrar um resultado de acurácia igual ou acima de 52%.**"]},{"cell_type":"code","metadata":{"id":"UFu2qNdF-Lyk"},"source":["test_acc = (best_net.predict(X_test) == y_test).mean()\n","print('Test accuracy: ', test_acc)"],"execution_count":null,"outputs":[]}]}